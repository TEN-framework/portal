---
title: Code Realtime Voice Agent Using Node.js
description: Using Node.js to build a real-time voice agent with TEN Framework.
author: Plutoless
date: 2025-08-28
articleLabel: use case
---

## Build a Voice Assistant with Node.js on TEN Framework

The TEN Framework makes it possible to build **real-time, low-latency voice assistants** that combine speech recognition, large language models, and text-to-speech â€” all orchestrated through a single pipeline.

In this tutorial, weâ€™ll show you how to use **Node.js** to create a voice assistant with TEN.
The best part? You donâ€™t need to reimplement ASR, LLM, or TTS in Node. You can **reuse Python or C++ extensions** for those, and just focus on writing the **main pipeline and business logic** in Node.js.

---

## Why Node.js with TEN?

TEN Framework is designed for **modular, cross-language development**:

* **RTC-first pipeline** â†’ audio/video/data flows are real-time and low-latency.
* **Cross-language extensions** â†’ use ASR in Python, TTS in C++, LLM in Go, etc.
* **Unified orchestration** â†’ Node.js just needs to implement the **main extension**, which orchestrates all components.

This means **your business logic lives in JavaScript/TypeScript**, while the heavy lifting is done by optimized extensions.

---

## Project Structure

You donâ€™t need to set everything up from scratch â€” TEN Framework already provides a **ready-to-use Node.js voice assistant example** in the repository.

ðŸ‘‰ You can find it here: [**voice-assistant-nodejs example on GitHub**](https://github.com/TEN-framework/ten-framework/tree/main/ai_agents/agents/examples/voice-assistant-nodejs)

The folder layout looks like this (mirroring what youâ€™ll find on GitHub):

```
.
â”œâ”€â”€ index.ts            â†’ MainControlExtension (Node.js pipeline logic)
â”œâ”€â”€ helper.ts           â†’ Utilities for sending Cmd/Data
â””â”€â”€ agent/
    â”œâ”€â”€ agent.ts        â†’ Event queue and orchestration
    â”œâ”€â”€ events.ts       â†’ Typed events (ASR, LLM, Tools, User)
    â”œâ”€â”€ llm_exec.ts     â†’ Handles LLM requests/responses
    â””â”€â”€ struct.ts       â†’ Zod schemas for message validation
```

This example shows how to implement the **main extension** in Node.js while reusing existing ASR, LLM, and TTS extensions written in Python or C++.

---

## Getting Started

We recommend following the [**official Getting Started guide**](https://theten.ai/docs/ten_agent/getting_started) for the basic setup steps (installations, API keys, environment, Docker, etc.).

âš ï¸ **Note:** When you reach the step to build the agent with `task use`, make sure to select the **Node.js voice assistant example**:

```bash
task use AGENT=agents/examples/voice-assistant-nodejs
```

This ensures youâ€™re running the **Node.js pipeline version**, while still reusing Python/C++ extensions for ASR, LLM, and TTS.

---

## The Main Extension

[`index.ts`](./index.ts) defines the **MainControlExtension**, your Node.js entry point. It wires the conversation loop together by reacting to runtime events and sending outputs to the right destinations.

Hereâ€™s how it works, split into its four core parts:

---

### 1. Greeting on User Join

When the **first user joins**, the extension greets them automatically.
It sends the configured greeting both to **TTS** (so the user hears it) and to the **transcript collector** (so it appears in the conversation history).

```ts
this.agent.on(UserJoinedEvent, async () => {
  this.joinedUserCount++;
  if (this.joinedUserCount === 1) {
    await this._send_to_tts(this.config.greeting, true);
    await this._send_transcript("assistant", this.config.greeting, true, 100);
  }
});
```

ðŸ‘‰ This makes sure your assistant always opens with a warm welcome.

---

### 2. Processing ASR Results

When speech recognition (ASR) emits results, the extension:

* Tracks the session/stream IDs.
* Issues an **interrupt** if the input is long or final, to stop ongoing LLM/TTS.
* Queues final user text into the LLM input pipeline.
* Sends the recognized transcript to the collector.

```ts
this.agent.on(ASRResultEvent, async (event) => {
  this.session_id = String(event.metadata?.session_id ?? "100");
  const stream_id = Number(this.session_id) || 0;

  if (!event.text) return;

  if (event.final || event.text.length > 2) {
    await this._interrupt();
  }

  if (event.final) {
    this.turn_id += 1;
    await this.agent.queueLLMInput(event.text);
  }

  await this._send_transcript("user", event.text, event.final, stream_id);
});
```

ðŸ‘‰ This is how **spoken input** gets turned into **LLM prompts**.

---

### 3. Handling LLM Results

When the LLM responds, the extension:

* Splits streaming deltas into complete sentences using `parseSentences`.
* Sends each sentence fragment immediately to **TTS**.
* For every message or reasoning chunk, forwards the transcript to the collector.

```ts
this.agent.on(LLMResponseEvent, async (event) => {
  if (!event.is_final && event.kind === "message") {
    const [sentences, remainText] = parseSentences(this.sentenceFragment, event.delta);
    this.sentenceFragment = remainText;
    for (const sentence of sentences) {
      await this._send_to_tts(sentence, false);
    }
  }

  const dataType = event.kind === "message" ? "text" : "reasoning";
  await this._send_transcript(
    "assistant", event.content, event.is_final, 100, dataType
  );
});
```

ðŸ‘‰ This enables **real-time speech synthesis** â€” users hear the assistant **while itâ€™s still thinking**.

---

### 4. Transcript Handling

All ASR and LLM text eventually flows through `_send_transcript`, which normalizes it into a structured format for the `message_collector`.

```ts
private async _send_transcript(
  role: string,
  text: string,
  final: boolean,
  stream_id: number,
  data_type: "text" | "reasoning" = "text",
) {
  await sendData(this.tenEnv, "message", "message_collector", {
    role,
    text,
    is_final: final,
    stream_id,
    data_type,
    text_ts: Date.now(),
  });
}
```

ðŸ‘‰ This ensures every utterance (user or assistant) is consistently logged for UI display, debugging, or analytics.

---

With TEN Framework, building a voice assistant in Node.js is about **writing orchestration and business logic** â€” not reinventing ASR, LLM, or TTS.

You can:

* **Reuse existing extensions** in Python/C++.
* **Keep your business pipeline and tools in Node.js**.
* **Deliver real-time voice assistants** with minimal code.

TEN brings the **best of both worlds**: cross-language extensibility and RTC-first performance.

---

## Test It Out

Now that youâ€™ve set everything up:

1. Follow the [Getting Started guide](https://theten.ai/docs/ten_agent/getting_started).
2. Use the Node.js agent:

```bash
task use AGENT=agents/examples/voice-assistant-nodejs
task run
```
<Callout title="Node.js extension update">
  Changing files in Node.js extension requires a build step. Run `task build` to rebuild all Node.js extensions.
</Callout>

3. Connect with playground at http://localhost:3000 or test it out in TMAN Designer.
4. Start speaking â€” your **Node.js pipeline** will orchestrate the flow.

âœ¨ Thatâ€™s it â€” you now have a working **voice assistant powered by Node.js on TEN Framework**!